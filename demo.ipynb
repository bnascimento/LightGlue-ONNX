{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGlue-ONNX Demo\n",
    "This demo shows how to export LightGlue to ONNX and perform inference with ONNXRuntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export import export_onnx\n",
    "from infer import infer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Export\n",
    "In this example we use SuperPoint features combined with LightGlue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_type = \"superpoint\"  # \"disk\"\n",
    "extractor_path = f\"weights/{extractor_type}.onnx\"\n",
    "lightglue_path = f\"weights/{extractor_type}_lightglue.onnx\"\n",
    "\n",
    "export_onnx(\n",
    "    extractor_type=extractor_type,\n",
    "    extractor_path=extractor_path,\n",
    "    lightglue_path=lightglue_path,\n",
    "    dynamic=True,  # for dynamic image size\n",
    "    max_num_keypoints=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_kpts0, m_kpts1 = infer(\n",
    "    img_paths=[\"assets/tilt_image.png\", \"assets/tile_image.png\"],\n",
    "    extractor_type=extractor_type,\n",
    "    extractor_path=extractor_path,\n",
    "    lightglue_path=lightglue_path,\n",
    "    img_size=512,\n",
    "    viz=True,\n",
    "    trt=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightglue import LightGlue, SuperPoint\n",
    "from lightglue.utils import load_image, read_image, numpy_image_to_torch, rbd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "# SuperPoint+LightGlue\n",
    "extractor = SuperPoint(max_keypoints=3840, detection_threshold=0.0005).eval().to(device)  # load the extractor\n",
    "matcher = LightGlue(features='superpoint', depth_confidence=0.99, width_confidence=0.99, filter_threshold=0.001).eval().to(device)  # load the matcher\n",
    "\n",
    "img0 = read_image('assets/sacre_coeur1.jpg')\n",
    "img1 = read_image('assets/sacre_coeur2.jpg')\n",
    "\n",
    "\n",
    "start_clock = time.time()\n",
    "\n",
    "# extract local features\n",
    "image0 = numpy_image_to_torch(img0).to(device)\n",
    "feats0 = extractor.extract(image0)\n",
    "\n",
    "end_clock = time.time()\n",
    "time_diff = end_clock - start_clock\n",
    "print(f\"Execution Time: {time_diff:.4f} seconds\")\n",
    "\n",
    "start_clock = time.time()\n",
    "\n",
    "# extract local features\n",
    "image1 = numpy_image_to_torch(img1).to(device)\n",
    "feats1 = extractor.extract(image1)\n",
    "\n",
    "end_clock = time.time()\n",
    "time_diff = end_clock - start_clock\n",
    "print(f\"Execution Time: {time_diff:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# match the features\n",
    "start_clock = time.time()\n",
    "matches01 = matcher({'image0': feats0, 'image1': feats1})\n",
    "feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]  # remove batch dimension\n",
    "\n",
    "\n",
    "matches = matches01['matches']  # indices with shape (K,2)\n",
    "print(f\"Number of matches: {matches.shape[0]}\")\n",
    "points0 = feats0['keypoints'][matches[..., 0]]  # coordinates in image #0, shape (K,2)\n",
    "points1 = feats1['keypoints'][matches[..., 1]]  # coordinates in image #1, shape (K,2)\n",
    "\n",
    "end_clock = time.time()\n",
    "time_diff = end_clock - start_clock\n",
    "print(f\"Execution Time: {time_diff:.4f} seconds\")\n",
    "        \n",
    "# Convert points to OpenCV KeyPoints\n",
    "img0_kpts = [cv2.KeyPoint(p[0], p[1], 1) for p in feats0['keypoints'].cpu().numpy()]\n",
    "img1_kpts = [cv2.KeyPoint(p[0], p[1], 1) for p in feats1['keypoints'].cpu().numpy()]\n",
    "\n",
    "# Convert matches\n",
    "good_matches = [cv2.DMatch(int(m[0]), int(m[1]), 0) for m in matches]\n",
    "\n",
    "# Draw the matches\n",
    "matched_image = cv2.drawMatches(\n",
    "    img0, img0_kpts,  # Image 1 and its keypoints\n",
    "    img1, img1_kpts,  # Image 2 and its keypoints\n",
    "    good_matches,  # Matches between keypoints\n",
    "    None,  # Output image, pass None to create automatically\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS  # Draw only matched points\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(matched_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "cv2.imwrite('assets/matched_image_coracao.png', cv2.cvtColor(matched_image, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from lightglue_dynamo import viz\n",
    "from lightglue_dynamo.preprocessors import SuperPointPreprocessor\n",
    "\n",
    "model_path = \"weights/superpoint_lightglue_pipeline.fp16.onnx\"\n",
    "output_path = \"matches_onnx.png\"\n",
    "width = 1024\n",
    "height = 1024\n",
    "fp16 = True\n",
    "profile = True\n",
    "device = \"cuda\"\n",
    "\n",
    "#raw_images = [cv2.resize(cv2.imread(str(i)), (width, height)) for i in raw_images]\n",
    "\n",
    "img1 = cv2.imread('assets/sacre_coeur1.jpg')\n",
    "img2 = cv2.imread('assets/sacre_coeur2.jpg')\n",
    "\n",
    "img1 = cv2.resize(img1, (width, height))\n",
    "img2 = cv2.resize(img2, (width, height))\n",
    "raw_images = [img1, img2]\n",
    "images = np.stack(raw_images)\n",
    "images = SuperPointPreprocessor.preprocess(images)\n",
    "images = images.astype(np.float16 if fp16 and device != \"tensorrt\" else np.float32)\n",
    "session_options = ort.SessionOptions()\n",
    "#session_options.enable_profiling = profile\n",
    "providers = [(\"CPUExecutionProvider\", {})]\n",
    "if device == \"cuda\":\n",
    "    providers.insert(0, (\"CUDAExecutionProvider\", {}))\n",
    "elif device == \"tensorrt\":\n",
    "    providers.insert(0, (\"CUDAExecutionProvider\", {}))\n",
    "    providers.insert(\n",
    "        0,\n",
    "        (\n",
    "            \"TensorrtExecutionProvider\",\n",
    "            {\n",
    "                \"trt_engine_cache_enable\": True,\n",
    "                \"trt_engine_cache_path\": \"weights/.trtcache_engines\",\n",
    "                \"trt_timing_cache_enable\": True,\n",
    "                \"trt_timing_cache_path\": \"weights/.trtcache_timings\",\n",
    "                \"trt_fp16_enable\": fp16,\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "elif device == \"openvino\":\n",
    "    providers.insert(0, (\"OpenVINOExecutionProvider\", {}))\n",
    "session = ort.InferenceSession(model_path, session_options, providers)\n",
    "for _ in range(4 if profile else 1):\n",
    "    \n",
    "    try:\n",
    "        start_clock = time.time()\n",
    "        \n",
    "        keypoints, matches, mscores = session.run(None, {\"images\": images})\n",
    "        matched_points0 = keypoints[0][matches[..., 1]]\n",
    "        matched_points1 = keypoints[1][matches[..., 2]]\n",
    "        end_clock = time.time()\n",
    "        time_diff = end_clock - start_clock\n",
    "        print(f\"Execution Time: {time_diff:.4f} seconds\")\n",
    "    #exception oom error  \n",
    "    except MemoryError as e:\n",
    "        print(f'OOM error encountered: {e}')\n",
    "        continue\n",
    "\n",
    "viz.plot_images(raw_images)\n",
    "viz.plot_matches(matched_points0, matched_points1, color=\"lime\", lw=0.2)\n",
    "if output_path is None:\n",
    "    viz.plt.show()\n",
    "else:\n",
    "    viz.save_plot(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE POLYGRAPHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import typer\n",
    "from polygraphy.backend.common import BytesFromPath\n",
    "from polygraphy.backend.trt import (\n",
    "    EngineFromBytes,\n",
    "    TrtRunner,\n",
    ")\n",
    "from lightglue_dynamo import viz\n",
    "from lightglue_dynamo.preprocessors import SuperPointPreprocessor\n",
    "\n",
    "model_path = \"weights/superpoint_lightglue_pipeline.fp16.engine\"\n",
    "output_path = \"matches_polygraphy.png\"\n",
    "width = 1024\n",
    "height = 1024\n",
    "fp16 = True\n",
    "profile = True\n",
    "\n",
    "img1 = cv2.imread('assets/sacre_coeur1.jpg')\n",
    "img2 = cv2.imread('assets/sacre_coeur2.jpg')\n",
    "img1 = cv2.resize(img1, (width, height))\n",
    "img2 = cv2.resize(img2, (width, height))\n",
    "raw_images = [img1, img2]\n",
    "images = np.stack(raw_images)\n",
    "images = SuperPointPreprocessor.preprocess(images)\n",
    "images = images.astype(np.float16 if fp16 else np.float32)\n",
    "# Build TensorRT engine\n",
    "build_engine = EngineFromBytes(BytesFromPath(str(model_path)))\n",
    "runner = TrtRunner(build_engine)\n",
    "runner.activate()\n",
    "        \n",
    "for _ in range(10 if profile else 1):  # Warm-up if profiling\n",
    "    outputs = runner.infer(feed_dict={\"images\": images})\n",
    "    keypoints, matches, mscores = outputs[\"keypoints\"], outputs[\"matches\"], outputs[\"mscores\"]  # noqa: F841\n",
    "    print(f\"Matches size: {matches.shape}\")\n",
    "        \n",
    "    if profile:\n",
    "        typer.echo(f\"Inference Time: {runner.last_inference_time():.3f} s\")\n",
    "        \n",
    "viz.plot_images(raw_images)\n",
    "viz.plot_matches(keypoints[0][matches[..., 1]], keypoints[1][matches[..., 2]], color=\"lime\", lw=0.2)\n",
    "if output_path is None:\n",
    "    viz.plt.show()\n",
    "else:\n",
    "    viz.save_plot(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.func import jvp, jacfwd\n",
    "\n",
    "# Example: residual function r(x)\n",
    "def residual_func(x):\n",
    "    return torch.sin(x.sum()) + torch.arange(10000.0, device=x.device) * x.sum()\n",
    "\n",
    "# Compute J^T J efficiently without storing J\n",
    "def compute_jtj_no_storage(residual_func, params):\n",
    "    N = params.numel()  # Number of parameters\n",
    "    JTJ = torch.zeros((N, N), dtype=params.dtype, device=params.device)  # Initialize J^T J\n",
    "\n",
    "    # Standard basis vectors\n",
    "    identity = torch.eye(N, dtype=params.dtype, device=params.device)\n",
    "    for i in range(N):\n",
    "        e_i = identity[i]\n",
    "        e_i[i] = 1.0\n",
    "\n",
    "        # Compute J @ e_i (i-th column of J)\n",
    "        _, jvp_result = jvp(residual_func, (params,), (e_i,))\n",
    "        \n",
    "        # Accumulate J^T J for the upper triangular part\n",
    "        JTJ[i, i:] += (jvp_result.unsqueeze(0) @ jvp_result.unsqueeze(1))[0, :N - i]\n",
    "\n",
    "    # Copy the upper triangular part to the lower triangular part\n",
    "    i_lower = torch.tril_indices(N, N, -1)\n",
    "    JTJ[i_lower[0], i_lower[1]] = JTJ.T[i_lower[0], i_lower[1]]\n",
    "\n",
    "    return JTJ\n",
    "\n",
    "# Compare performance and memory usage\n",
    "def compare_methods(residual_func, params):\n",
    "    # Measure no-storage method\n",
    "    start_time = time.perf_counter()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    JTJ_no_storage = compute_jtj_no_storage(residual_func, params)\n",
    "    elapsed_no_storage = time.perf_counter() - start_time\n",
    "\n",
    "    # Measure full Jacobian method\n",
    "    start_time = time.perf_counter()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    J = jacfwd(residual_func, argnums=0)(params)\n",
    "    JTJ_full = J.T @ J\n",
    "    elapsed_full = time.perf_counter() - start_time\n",
    "\n",
    "    # Check memory usage (GPU only)\n",
    "    if torch.cuda.is_available():\n",
    "        memory_no_storage = torch.cuda.memory_allocated()\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_full = torch.cuda.memory_allocated()\n",
    "        return elapsed_no_storage, elapsed_full, memory_no_storage, memory_full\n",
    "    else:\n",
    "        return elapsed_no_storage, elapsed_full\n",
    "\n",
    "# Example usage\n",
    "params = torch.randn(500, requires_grad=True)  # 500 parameters (N=500)\n",
    "results = compare_methods(residual_func, params)\n",
    "\n",
    "# Display results\n",
    "if len(results) == 4:  # GPU with memory tracking\n",
    "    elapsed_no_storage, elapsed_full, memory_no_storage, memory_full = results\n",
    "    print(f\"No-storage method: {elapsed_no_storage:.4f}s, {memory_no_storage / 1e6:.2f} MB\")\n",
    "    print(f\"Full Jacobian method: {elapsed_full:.4f}s, {memory_full / 1e6:.2f} MB\")\n",
    "else:  # CPU only\n",
    "    elapsed_no_storage, elapsed_full = results\n",
    "    print(f\"No-storage method: {elapsed_no_storage:.4f}s\")\n",
    "    print(f\"Full Jacobian method: {elapsed_full:.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
